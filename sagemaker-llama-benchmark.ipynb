{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28ea83b-8a9b-4a20-abc0-37948cdfd2d7",
   "metadata": {},
   "source": [
    "# Load Testing Llama3 on SageMaker RT Inference With LLMPerf\n",
    "\n",
    "In this notebook we'll take the [LLMPerf](https://github.com/ray-project/llmperf) repo and showcase how you can use it to load test Llama3 deployed on SageMaker JumpStart RT-Inference.. LLM Load Testing is a little different then our traditional load testing we did for ML models, we'll specifically look at metrics such as:\n",
    "\n",
    "- Time to First Token\n",
    "- Token Throughput (Tokens per Second)\n",
    "\n",
    "Along with our usual requests per minutes (RPM), but these more granular metrics give a more accurate picture in terms of our LLM performance especially when using an API that bills based off of the token input the model is processing.\n",
    "\n",
    "## Additional Resources/Credits\n",
    "- [JumpStart Starter Guide](https://www.youtube.com/watch?v=c0ASHUm3BwA&t=636s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7b14d-2996-4093-af98-c2ad52ebc0cb",
   "metadata": {},
   "source": [
    "## Llama Deployment via JumpStart\n",
    "You can also optionally skip this and just bring your own endpoint, if you would like to specify hardware ensure to specify an instance type in the deployment params here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e889b-e9ce-412a-bb2b-ef9ecd0be57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id = \"meta-textgeneration-llama-3-8b\")\n",
    "predictor = model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37983de1-ef7f-4e62-b5b8-49b96850eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "sm_client = boto3.client(\"sagemaker-runtime\", region_name=\"us-east-1\")\n",
    "payload = {\n",
    "    \"inputs\": \"Who is Roger Federer?\",\n",
    "    \"parameters\": {\"max_new_tokens\":256, \"top_p\":0.9, \"temperature\":0.6}\n",
    "}\n",
    "\n",
    "response = sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "#print(response)\n",
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352cb15-b91f-4e3d-b668-257f927eb932",
   "metadata": {},
   "source": [
    "## LLMPerf Setup\n",
    "Our notebook setting is a conda_python3 kernel and ml.g5.12xlarge SageMaker Notebook Instance, note dependencies versions might change depending on the environment you're in, the installations are configured for this specific environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97da1e0-51fd-40e0-9780-5b9c884119e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setuptools==65.5.1 --quiet\n",
    "!git clone https://github.com/ray-project/llmperf.git\n",
    "!cd llmperf; pip install -e . --quiet; cd ..\n",
    "!pip install pydantic -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b634d-9a46-4924-b9dd-783c0e87a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from litellm import completion\n",
    "import litellm\n",
    "#litellm._turn_on_debug()\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"Enter Access Key\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"Enter Secret Access Key\"\n",
    "os.environ[\"AWS_REGION_NAME\"] = \"us-east-1\" #update region if needed\n",
    "\n",
    "response = completion(\n",
    "            model=f\"sagemaker/{endpoint_name}\", \n",
    "            messages=[{ \"content\": \"Who is Roger Federer?\",\"role\": \"user\"}],\n",
    "        )\n",
    "output = response.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d6fab-90b6-4dbf-ad14-28671c6c470c",
   "metadata": {},
   "source": [
    "## LLMPerf Benchmark\n",
    "\n",
    "Here we utilize the following LLMPerf script: https://github.com/ray-project/llmperf/blob/main/token_benchmark_ray.py to configure our load test. You can adjust the input and output token sizes depending on your use-cases. Other parameters you can toggle include the number of concurrent requests and test duration. We've configured the test to run for 5 minutes and after conclusion you should see the results displayed and a directory called <b>sagemaker-outputs</b> with the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3da857-d29e-475e-b344-0a69e09dd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "python llmperf/token_benchmark_ray.py \\\n",
    "    --model sagemaker/<enter ep name here> \\\n",
    "    --mean-input-tokens 1024 \\\n",
    "    --stddev-input-tokens 200 \\\n",
    "    --mean-output-tokens 1024 \\\n",
    "    --stddev-output-tokens 200 \\\n",
    "    --max-num-completed-requests 20 \\\n",
    "    --num-concurrent-requests 1 \\\n",
    "    --timeout 300 \\\n",
    "    --llm-api litellm \\\n",
    "    --results-dir sagemaker-outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
