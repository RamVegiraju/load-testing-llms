{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b563f6-63cc-406f-a361-827488f58862",
   "metadata": {},
   "source": [
    "# Load Testing Bedrock Claude Sonnet With LLMPerf\n",
    "\n",
    "In this notebook we'll take the [LLMPerf](https://github.com/ray-project/llmperf) repo and showcase how you can use it to load test Bedrock Claude 3 Sonnet. LLM Load Testing is a little different then our traditional load testing we did for ML models, we'll specifically look at metrics such as:\n",
    "\n",
    "- Time to First Token\n",
    "- Token Throughput (Tokens per Second)\n",
    "\n",
    "Along with our usual requests per minutes (RPM), but these more granular metrics give a more accurate picture in terms of our LLM performance especially when using an API that bills based off of the token input the model is processing.\n",
    "\n",
    "## Additional Resources/Credits\n",
    "- [Bedrock Starter Guide](https://www.youtube.com/watch?v=8aMJUV0qhow&t=3s)\n",
    "- [Load Testing Custom Models Bedrock](https://aws.amazon.com/blogs/machine-learning/benchmarking-customized-models-on-amazon-bedrock-using-llmperf-and-litellm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643a175-4611-4466-9d7f-fecdf74f42ee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Our notebook setting is a conda_python3 kernel and ml.g5.12xlarge SageMaker Notebook Instance, note dependencies versions might change depending on the environment you're in, the installations are configured for this specific environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d61cc-f218-462a-bc81-46fdaf970c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setuptools==65.5.1 --quiet\n",
    "!git clone https://github.com/ray-project/llmperf.git\n",
    "!cd llmperf; pip install -e . --quiet; cd ..\n",
    "!pip install pydantic -U --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50831f-a732-458c-a16d-872c85a05362",
   "metadata": {},
   "source": [
    "## LiteLLM\n",
    "[LiteLLM](https://github.com/BerriAI/litellm) helps you invoke different Model Providers in a singular unified format making it simple to test across different LLM Providers/Models. In this case we can test to see how it works with Bedrock, this is also natively integrated with LLMPerf and simplifies our load testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f32c64-de4a-4f82-9c63-357f6cd632bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"Enter your access key ID\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"Enter your secret access key\"\n",
    "os.environ[\"AWS_REGION_NAME\"] = \"us-east-1\"\n",
    "\n",
    "response = completion(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    messages=[{ \"content\": \"Who is Roger Federer?\",\"role\": \"user\"}]\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8e1a9-92e7-42e2-bf18-d4c5c99c6206",
   "metadata": {},
   "source": [
    "## LLMPerf Benchmark\n",
    "\n",
    "Here we utilize the following LLMPerf script: https://github.com/ray-project/llmperf/blob/main/token_benchmark_ray.py to configure our load test. You can adjust the input and output token sizes depending on your use-cases. Other parameters you can toggle include the number of concurrent requests and test duration. We've configured the test to run for 5 minutes and after conclusion you should see the results displayed and a directory called <b>bedrock-outputs</b> with the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a9530-ff44-41f6-b543-5cd868e89d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "python llmperf/token_benchmark_ray.py \\\n",
    "    --model bedrock/anthropic.claude-3-sonnet-20240229-v1:0 \\\n",
    "    --mean-input-tokens 1024 \\\n",
    "    --stddev-input-tokens 200 \\\n",
    "    --mean-output-tokens 1024 \\\n",
    "    --stddev-output-tokens 200 \\\n",
    "    --max-num-completed-requests 30 \\\n",
    "    --num-concurrent-requests 1 \\\n",
    "    --timeout 300 \\\n",
    "    --llm-api litellm \\\n",
    "    --results-dir bedrock-outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1ac3b-392a-4e10-b98b-c298b5e049d0",
   "metadata": {},
   "source": [
    "### Display Summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016465f-76ca-4c3b-bd26-d67381f41b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load JSON files\n",
    "individual_path = Path(\"bedrock-outputs/bedrock-anthropic-claude-3-sonnet-20240229-v1-0_1024_1024_individual_responses.json\")\n",
    "summary_path = Path(\"bedrock-outputs/bedrock-anthropic-claude-3-sonnet-20240229-v1-0_1024_1024_summary.json\")\n",
    "\n",
    "with open(individual_path, \"r\") as f:\n",
    "    individual_data = json.load(f)\n",
    "\n",
    "with open(summary_path, \"r\") as f:\n",
    "    summary_data = json.load(f)\n",
    "\n",
    "# Print summary metrics\n",
    "df = pd.DataFrame(individual_data)\n",
    "summary_metrics = {\n",
    "    \"Model\": summary_data.get(\"model\"),\n",
    "    \"Mean Input Tokens\": summary_data.get(\"mean_input_tokens\"),\n",
    "    \"Stddev Input Tokens\": summary_data.get(\"stddev_input_tokens\"),\n",
    "    \"Mean Output Tokens\": summary_data.get(\"mean_output_tokens\"),\n",
    "    \"Stddev Output Tokens\": summary_data.get(\"stddev_output_tokens\"),\n",
    "    \"Mean TTFT (s)\": summary_data.get(\"results_ttft_s_mean\"),\n",
    "    \"Mean Inter-token Latency (s)\": summary_data.get(\"results_inter_token_latency_s_mean\"),\n",
    "    \"Mean Output Throughput (tokens/s)\": summary_data.get(\"results_mean_output_throughput_token_per_s\"),\n",
    "    \"Completed Requests\": summary_data.get(\"results_num_completed_requests\"),\n",
    "    \"Error Rate\": summary_data.get(\"results_error_rate\")\n",
    "}\n",
    "print(\"Claude 3 Sonnet - Performance Summary:\\n\")\n",
    "for k, v in summary_metrics.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
